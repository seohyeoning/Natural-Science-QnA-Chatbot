import pickle
import re
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity
from fuzzywuzzy import fuzz
import pandas as pd

def normalize_text(text):
    """ë¬¸ìì—´ì˜ ê³µë°± ë° íŠ¹ìˆ˜ë¬¸ìë¥¼ ì œê±°í•˜ê³  ì†Œë¬¸ìë¡œ ë³€í™˜"""
    text = re.sub(r'\W+', ' ', text)
    return text.strip().lower()

def is_correct(predicted, actual, similarity_threshold=100):
    """
    predictedì™€ actual ë¬¸ìì—´ì„ fuzzy matchingí•˜ì—¬ ìœ ì‚¬ë„ê°€ 
    similarity_threshold ì´ìƒì´ë©´ True ë°˜í™˜
    """
    predicted = normalize_text(predicted)
    actual = normalize_text(actual)
    similarity_score = fuzz.partial_ratio(predicted, actual)
    return similarity_score >= similarity_threshold

def evaluate_faq_bot_tfidf(model_path, test_pairs, k_values=[1, 3, 5], similarity_threshold=100):
    """
    FAQ ì±—ë´‡ì˜ ì„±ëŠ¥ì„ í‰ê°€í•©ë‹ˆë‹¤ (TF-IDF ê¸°ë°˜, ì˜ˆì¸¡ FAQ ì§ˆë¬¸ vs. ì›ë³¸ FAQ ì§ˆë¬¸ ë¹„êµ).

    ëª¨ë¸ íŒŒì¼ì€ TF-IDF vectorizer, TF-IDF í–‰ë ¬, ê·¸ë¦¬ê³  FAQ ì§ˆë¬¸ì´ í¬í•¨ëœ DataFrameì„
    pickle íŒŒì¼ì— ì €ì¥ë˜ì–´ ìˆì–´ì•¼ í•©ë‹ˆë‹¤.
    
    Args:
        model_path (str): ì €ì¥ëœ TF-IDF ëª¨ë¸ íŒŒì¼ ê²½ë¡œ (pickle íŒŒì¼).
        test_pairs (list of tuple): (original_question, similar_question) í˜•íƒœì˜ í…ŒìŠ¤íŠ¸ ë°ì´í„°.
            - original_question: FAQì— ë“±ë¡ëœ ì •ë‹µ ì§ˆë¬¸(ground truth)
            - similar_question: ì‚¬ìš©ìê°€ ì…ë ¥í•œ ì§ˆë¬¸(ë³€í˜•)
        k_values (list): Top-K í‰ê°€ë¥¼ ìœ„í•œ K ê°’ ë¦¬ìŠ¤íŠ¸.
        similarity_threshold (int): fuzzy matching ìœ ì‚¬ë„ ì„ê³„ê°’ (0~100).

    Returns:
        dict: MRR ë° Top-K ì •í™•ë„ ê²°ê³¼.
    """
    # TF-IDF ëª¨ë¸ ë¡œë“œ (vectorizer, tfidf_matrix, ê·¸ë¦¬ê³  FAQ ì§ˆë¬¸ì´ ë‹´ê¸´ DataFrame)
    with open(model_path, "rb") as f:
        loaded_data = pickle.load(f)
    vectorizer = loaded_data["vectorizer"]
    tfidf_matrix = loaded_data["tfidf_matrix"]
    df = loaded_data["df"]  # DataFrameì— FAQ ì§ˆë¬¸ì´ ì €ì¥ë˜ì–´ ìˆë‹¤ê³  ê°€ì • (ì»¬ëŸ¼ëª…: "question")
    questions = df["question"].tolist()  # FAQ ì§ˆë¬¸ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜

    print("âœ… ì €ì¥ëœ TF-IDF ëª¨ë¸ì„ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")

    ranks = []
    top_k_accuracies = {k: 0 for k in k_values}

    for original_question, similar_question in test_pairs:
        # ì‚¬ìš©ìê°€ ì…ë ¥í•œ ì§ˆë¬¸(ë³€í˜•)ì„ ë²¡í„°í™”
        query_vector = vectorizer.transform([similar_question])
        # TF-IDF í–‰ë ¬ê³¼ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
        cosine_scores = cosine_similarity(query_vector, tfidf_matrix)[0]
        
        # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ë‚´ë¦¼ì°¨ìˆœìœ¼ë¡œ ì •ë ¬í•˜ì—¬ Top-k ì¸ë±ìŠ¤ ì¶”ì¶œ
        sorted_indices = np.argsort(cosine_scores)[::-1]
        max_k = max(k_values)
        top_indices = sorted_indices[:max_k]
        
        # ì˜ˆì¸¡ëœ FAQ ì§ˆë¬¸ ë¦¬ìŠ¤íŠ¸ (ìƒìœ„ max_k ê°œ)
        predicted_questions = [questions[idx] for idx in top_indices]
        
        # ë””ë²„ê¹…ìš© ì¶œë ¥
        print(f"\nğŸ” User Query (similar_question): {similar_question}")
        for idx, score in zip(top_indices, cosine_scores[top_indices]):
            print(f"ğŸ”¹ Retrieved FAQ Question: {questions[idx]} (ì½”ì‚¬ì¸ ìœ ì‚¬ë„: {score:.4f})")
        
        # Rank ê³„ì‚°: ì˜ˆì¸¡ëœ FAQ ì§ˆë¬¸ê³¼ ì‹¤ì œ ì›ë³¸ ì§ˆë¬¸ ê°„ì˜ fuzzy matching í‰ê°€
        rank = next((i + 1 for i, pred_q in enumerate(predicted_questions) 
                     if is_correct(pred_q, original_question, similarity_threshold)), None)
        ranks.append(rank if rank else float('inf'))
        
        # Top-K ì •í™•ë„ ê³„ì‚°
        for k in k_values:
            if any(is_correct(pred_q, original_question, similarity_threshold) 
                   for pred_q in predicted_questions[:k]):
                top_k_accuracies[k] += 1

    # MRR ê³„ì‚°
    mrr = sum(1 / rank for rank in ranks if rank != float('inf')) / len(ranks)
    
    # Top-K ì •í™•ë„ ë¹„ìœ¨ ê³„ì‚°
    for k in k_values:
        top_k_accuracies[k] = top_k_accuracies[k] / len(test_pairs)
    
    results = {"MRR": mrr}
    results.update({f"Top-{k} Accuracy": acc for k, acc in top_k_accuracies.items()})
    return results

def load_test_pairs(file_path):
    """
    CSV íŒŒì¼ì—ì„œ í…ŒìŠ¤íŠ¸ í˜ì–´ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.
    CSV íŒŒì¼ì€ 'original_question' (FAQ ì›ë³¸ ì§ˆë¬¸)ì™€ 
    'similar_question' (ì‚¬ìš©ì ë³€í˜• ì§ˆë¬¸) ì—´ì„ í¬í•¨í•´ì•¼ í•©ë‹ˆë‹¤.
    """
    try:
        df = pd.read_csv(file_path, delimiter=",", quotechar='"', encoding="utf-8", on_bad_lines="skip")
    except UnicodeDecodeError:
        df = pd.read_csv(file_path, delimiter=",", quotechar='"', encoding="ISO-8859-1", on_bad_lines="skip")
    
    if 'original_question' not in df.columns or 'similar_question' not in df.columns:
        raise ValueError("CSV íŒŒì¼ì— 'original_question' ë° 'similar_question' ì—´ì´ í•„ìš”í•©ë‹ˆë‹¤.")
    
    test_pairs = list(zip(df['original_question'], df['similar_question']))
    return test_pairs

# ì‚¬ìš© ì˜ˆì‹œ
if __name__ == "__main__":
    test_pairs = load_test_pairs("Generated_Question_Pairs.csv")
    model_path = "tfidf_chatbot_model.pkl"  # TF-IDF ëª¨ë¸ íŒŒì¼ ê²½ë¡œ (DataFrame, vectorizer, tfidf_matrix í¬í•¨)
    results = evaluate_faq_bot_tfidf(model_path, test_pairs)
    
    print("\n=== FAQ Chatbot TF-IDF Performance ===")
    for metric, value in results.items():
        print(f"{metric}: {value:.4f}")
    
    print("ìœ ì‚¬ë„ ê¸°ë°˜ í‰ê°€ (ì˜ˆì¸¡ ì§ˆë¬¸ vs. ì›ë³¸ ì§ˆë¬¸)")
