import torch
import pickle
from sentence_transformers import SentenceTransformer, util
import pandas as pd
from fuzzywuzzy import fuzz
import re


def normalize_text(text):
    """ë¬¸ìì—´ì˜ ê³µë°± ë° íŠ¹ìˆ˜ë¬¸ìë¥¼ ì œê±°í•˜ê³  ì†Œë¬¸ìë¡œ ë³€í™˜"""
    text = re.sub(r'\W+', ' ', text)  # ëª¨ë“  íŠ¹ìˆ˜ë¬¸ìë¥¼ ê³µë°±ìœ¼ë¡œ ëŒ€ì²´
    return text.strip().lower()


def is_correct(predicted, actual, similarity_threshold=100):
    """ë¬¸ìì—´ì´ ì¼ì • ìˆ˜ì¤€ ì´ìƒ ìœ ì‚¬í•˜ë©´ True ë°˜í™˜"""
    predicted = normalize_text(predicted)
    actual = normalize_text(actual)

    # ë¬¸ìì—´ ìœ ì‚¬ë„ ê³„ì‚° (fuzz.partial_ratio ì‚¬ìš©)
    similarity_score = fuzz.partial_ratio(predicted, actual)
    return similarity_score >= similarity_threshold


def evaluate_faq_bot(model_path, test_pairs, k_values=[1, 3, 5]):
    """
    FAQ ì±—ë´‡ì˜ ì„±ëŠ¥ì„ í‰ê°€í•©ë‹ˆë‹¤.
    ì—¬ê¸°ì„œëŠ” ì˜ˆì¸¡ëœ FAQ ì§ˆë¬¸ê³¼ ì‹¤ì œ ì›ë³¸ ì§ˆë¬¸ì˜ ìœ ì‚¬ì„±ì„ ê¸°ì¤€ìœ¼ë¡œ í‰ê°€í•©ë‹ˆë‹¤.
    
    test_pairsëŠ” (original_question, similar_question) í˜•íƒœë¡œ,
    - original_question: FAQì— ë“±ë¡ëœ ì •ë‹µ ì§ˆë¬¸ (ground truth)
    - similar_question: ìœ ì €ê°€ ì…ë ¥í•œ ì§ˆë¬¸(ì›ë³¸ ì§ˆë¬¸ê³¼ ìœ ì‚¬í•œ ë³€í˜•)
    """
    # ëª¨ë¸ ë° FAQ ë°ì´í„°(ì§ˆë¬¸, ë‹µë³€, ì§ˆë¬¸ ì„ë² ë”©) ë¡œë“œ
    with open(model_path, "rb") as f:
        questions, answers, question_embeddings = pickle.load(f)

    # ì„ë² ë”© ìƒì„± ëª¨ë¸ ë¡œë“œ
    model = SentenceTransformer('all-MiniLM-L6-v2')

    # í‰ê°€ ë©”íŠ¸ë¦­ ì´ˆê¸°í™”
    ranks = []
    top_k_accuracies = {k: 0 for k in k_values}

    # í…ŒìŠ¤íŠ¸ ë£¨í”„: ê° í…ŒìŠ¤íŠ¸ í˜ì–´ì—ì„œ ìœ ì € ì§ˆë¬¸(similar_question)ì„ ì´ìš©í•´ FAQ ì§ˆë¬¸ì„ ê²€ìƒ‰
    for original_question, similar_question in test_pairs:
        # ìœ ì € ì§ˆë¬¸(ë³€í˜•ëœ ì§ˆë¬¸)ì„ ì„ë² ë”©ìœ¼ë¡œ ë³€í™˜
        query_embedding = model.encode(similar_question, convert_to_tensor=True)

        # FAQ ì§ˆë¬¸ ì„ë² ë”©ê³¼ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
        cosine_scores = util.cos_sim(query_embedding, question_embeddings)[0]

        # ê°€ì¥ ë†’ì€ ìœ ì‚¬ë„ë¥¼ ë³´ì´ëŠ” Top-k FAQ ì§ˆë¬¸ ì¸ë±ìŠ¤ ë°˜í™˜
        top_results = torch.topk(cosine_scores, k=max(k_values))

        # ì˜ˆì¸¡ëœ FAQ ì§ˆë¬¸ ë¦¬ìŠ¤íŠ¸ (questions ë¦¬ìŠ¤íŠ¸ì—ì„œ ê°€ì ¸ì˜´)
        predicted_questions = [questions[idx] for idx in top_results.indices.tolist()]

        # ë””ë²„ê¹…ìš© ì¶œë ¥
        print(f"\nğŸ” User Query (similar_question): {similar_question}")
        for idx, score in zip(top_results.indices.tolist(), top_results.values.tolist()):
            print(f"ğŸ”¹ Retrieved FAQ Question: {questions[idx]} (ìœ ì‚¬ë„: {score:.4f})")

        # Rank ê³„ì‚°: ì˜ˆì¸¡ëœ FAQ ì§ˆë¬¸ê³¼ ì‹¤ì œ ì›ë³¸ ì§ˆë¬¸(original_question)ì˜ ìœ ì‚¬ì„±ì„ ê¸°ì¤€ìœ¼ë¡œ í•¨
        rank = next((i + 1 for i, pred_q in enumerate(predicted_questions) if is_correct(pred_q, original_question)), None)
        ranks.append(rank if rank else float('inf'))

        # Top-K ì •í™•ë„ ê³„ì‚°
        for k in k_values:
            if any(is_correct(pred_q, original_question) for pred_q in predicted_questions[:k]):
                top_k_accuracies[k] += 1

    # MRR (Mean Reciprocal Rank) ê³„ì‚°
    mrr = sum(1 / rank for rank in ranks if rank != float('inf')) / len(ranks)

    # Top-K ì •í™•ë„ ë¹„ìœ¨ ê³„ì‚°
    for k in k_values:
        top_k_accuracies[k] = top_k_accuracies[k] / len(test_pairs)

    # ê²°ê³¼ ë°˜í™˜
    results = {"MRR": mrr}
    results.update({f"Top-{k} Accuracy": acc for k, acc in top_k_accuracies.items()})
    return results


def load_test_pairs(file_path):
    """CSV íŒŒì¼ì—ì„œ í…ŒìŠ¤íŠ¸ í˜ì–´ë¥¼ ë¡œë“œí•˜ê³  ì „ì²˜ë¦¬ ìˆ˜í–‰
       CSV íŒŒì¼ì€ 'original_question' (ì›ë³¸ FAQ ì§ˆë¬¸)ì™€ 'similar_question' (ìœ ì € ì§ˆë¬¸ ë³€í˜•) ì—´ì„ í¬í•¨í•´ì•¼ í•©ë‹ˆë‹¤.
    """
    try:
        df = pd.read_csv(file_path, delimiter=",", quotechar='"', encoding="utf-8", on_bad_lines="skip")
    except UnicodeDecodeError:
        df = pd.read_csv(file_path, delimiter=",", quotechar='"', encoding="ISO-8859-1", on_bad_lines="skip")

    if 'original_question' not in df.columns or 'similar_question' not in df.columns:
        raise ValueError("CSV íŒŒì¼ì— 'original_question' ë° 'similar_question' ì—´ì´ í•„ìš”í•©ë‹ˆë‹¤.")

    test_pairs = list(zip(df['original_question'], df['similar_question']))
    return test_pairs


# ì‚¬ìš© ì˜ˆì‹œ
if __name__ == "__main__":
    # CSV íŒŒì¼ì—ì„œ í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ
    test_pairs = load_test_pairs("Generated_Question_Pairs.csv")

    # FAQ ëª¨ë¸ ê²½ë¡œ ì§€ì •
    model_path = "qna_model.pkl"

    # í‰ê°€ ì‹¤í–‰
    results = evaluate_faq_bot(model_path, test_pairs)

    # ê²°ê³¼ ì¶œë ¥
    print("\n=== FAQ Chatbot Performance ===")
    for metric, value in results.items():
        print(f"{metric}: {value:.4f}")
    
    print("ìœ ì‚¬ë„ ê¸°ë°˜ ì •í™•ë„ í‰ê°€ (ì˜ˆì¸¡ ì§ˆë¬¸ vs. ì›ë³¸ ì§ˆë¬¸)")
